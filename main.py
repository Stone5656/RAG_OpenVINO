import os
import time

# --- å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª ---
try:
    from langchain_community.llms import LlamaCpp
    from langchain_community.document_loaders import PyMuPDFLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_community.vectorstores import FAISS
    from langchain.chains import RetrievalQA
    from langchain.prompts import PromptTemplate
except ImportError:
    print("å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    print("ä»®æƒ³ç’°å¢ƒã‚’æœ‰åŠ¹åŒ–ã—ã¦ã€pip install ... ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    exit()


# --- 1. è¨­å®š ---
# å‚ç…§sã—ãŸã„PDFãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹
PDF_PATH = "./docs/ç„¡é¡Œã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ.pdf"
# ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰ã‚’ä¿å­˜ã™ã‚‹å ´æ‰€
VECTORSTORE_PATH = "faiss_index"
# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸGGUFãƒ¢ãƒ‡ãƒ«ã¸ã®ãƒ‘ã‚¹
MODEL_PATH = "./models/ELYZA-japanese-Llama-2-7b-fast-instruct-q2_K.gguf"
# æ—¥æœ¬èªã«å¼·ã„åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
EMBEDDING_MODEL = "intfloat/multilingual-e5-large"


# --- 2. PDFã‹ã‚‰ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’ä½œæˆã™ã‚‹é–¢æ•° ---
def create_vectorstore():
    """PDFã‚’èª­ã¿è¾¼ã¿ã€FAISSãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’ä½œæˆã—ã¦ä¿å­˜ã™ã‚‹"""
    if not os.path.exists(PDF_PATH):
        print(f"ã‚¨ãƒ©ãƒ¼: PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {PDF_PATH}")
        return False
        
    print("--- ã‚¹ãƒ†ãƒƒãƒ—1: PDFã‹ã‚‰ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’æ§‹ç¯‰ã—ã¾ã™ ---")
    
    print("PDFã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    loader = PyMuPDFLoader(PDF_PATH)
    documents = loader.load()
    
    print("ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ä¸­...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    texts = text_splitter.split_documents(documents)
    
    print("åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...ï¼ˆåˆå›ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰")
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
    
    print("ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆä¸­...")
    db = FAISS.from_documents(texts, embeddings)
    db.save_local(VECTORSTORE_PATH)
    
    print(f"--- ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã®æ§‹ç¯‰å®Œäº†ï¼ '{VECTORSTORE_PATH}'ã«ä¿å­˜ã—ã¾ã—ãŸ ---")
    return True

# --- 3. ãƒ¡ã‚¤ãƒ³ã®å®Ÿè¡Œéƒ¨åˆ† ---
def main():
    """ãƒ¡ã‚¤ãƒ³ã®å®Ÿè¡Œå‡¦ç†"""
    # ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ãŒå­˜åœ¨ã—ãªã„å ´åˆã®ã¿ä½œæˆã™ã‚‹
    if not os.path.exists(VECTORSTORE_PATH):
        if not create_vectorstore():
            return # PDFãŒãªã„å ´åˆã¯çµ‚äº†
            
    # --- ãƒ¢ãƒ‡ãƒ«ã¨RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æº–å‚™ ---
    print("\n--- ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ¢ãƒ‡ãƒ«ã¨RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æº–å‚™ã—ã¾ã™ ---")
    
    print("åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
    
    print("æ—¢å­˜ã®ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    db = FAISS.load_local(VECTORSTORE_PATH, embeddings, allow_dangerous_deserialization=True)
    retriever = db.as_retriever(search_kwargs={"k": 4}) 

    print("LLMã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...ï¼ˆPCã®æ€§èƒ½ã«ã‚ˆã£ã¦ã¯æ•°åˆ†ã‹ã‹ã‚Šã¾ã™ï¼‰")
    if not os.path.exists(MODEL_PATH):
        print(f"ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {MODEL_PATH}")
        return
        
    llm = LlamaCpp(
        model_path=MODEL_PATH,
        n_gpu_layers=0, # GPUã‚’ä½¿ã‚ãªã„å ´åˆã¯0ã€NVIDIA GPUã¯-1ã€AMD Radeonã¯-1ï¼ˆè¦ãƒ»å°‚ç”¨ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰
        n_batch=512,
        n_ctx=8192,
        f16_kv=True,
        verbose=False,
    )
    print("--- ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼ ---")

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å®šç¾©
    template = """
### Instruction:
ä»¥ä¸‹ã®ã€Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã€ã‚’æ³¨æ„æ·±ãèª­ã¿ã€ãã®æƒ…å ±ã ã‘ã«åŸºã¥ã„ã¦ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€ã«æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã«ç­”ãˆãŒãªã„å ´åˆã¯ã€ã€Œåˆ†ã‹ã‚Šã¾ã›ã‚“ã€ã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚

### Context:
{context}

### Question:
{question}

### Response:
"""

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ
    prompt = PromptTemplate(
        template=template, input_variables=["context", "question"]
    )
    
    # RAGãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆ (chain_type_kwargsã§ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æŒ‡å®š)
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt}
    )
    
    print("\næº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸã€‚PDFã«é–¢ã™ã‚‹è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    print("çµ‚äº†ã™ã‚‹ã«ã¯ 'exit' ã¾ãŸã¯ 'quit' ã¨å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    # --- è³ªå•å¿œç­”ãƒ«ãƒ¼ãƒ— ---
    while True:
        query = input("\n[è³ªå•] > ")
        if query.lower() in ['exit', 'quit']:
            break
        if not query.strip():
            continue

        start_time = time.time()
        print("ğŸ¤– å›ç­”ã‚’ç”Ÿæˆä¸­...")
        
        result = qa_chain.invoke(query)
        
        end_time = time.time()

        print("\n--- å›ç­” ---")
        print(result['result'].strip())
        print("-----------")
        print(f"ï¼ˆç”Ÿæˆæ™‚é–“: {end_time - start_time:.2f}ç§’ï¼‰")
        
        # --- å‚ç…§ã‚½ãƒ¼ã‚¹ ---
        print("\n--- å‚ç…§ã‚½ãƒ¼ã‚¹ ---")
        for doc in result['source_documents']:
            content_preview = doc.page_content[:150].strip().replace('\n', ' ')
            page_number = doc.metadata.get('page', 'N/A')
            print(f"ğŸ“„ ãƒšãƒ¼ã‚¸: {page_number}, å†…å®¹: {content_preview}...")
        print("------------------")

if __name__ == "__main__":
    main()