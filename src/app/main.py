# -*- coding: utf-8 -*-
"""
ã‚¢ãƒ—ãƒªçµ±åˆã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆï¼ˆStreamlitï¼‰

- å½¹å‰²:
    * åˆ†é›¢æ¸ˆã¿ã® UI / PDF / ãƒ¢ãƒ‡ãƒ« / ä¼šè©±å±¤ã‚’çµç·šã—ã€RAGå‹QA UI ã‚’æä¾›ã™ã‚‹
- æ–¹é‡:
    * pathlib.Path ã§ãƒ‘ã‚¹ç®¡ç†
    * å‹æ³¨é‡ˆã¯ from __future__ import annotations
    * UI ã¯ ui/ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€PDFã¯ pdf/ã€ãƒ¢ãƒ‡ãƒ«ã¯ model/ã€ä¼šè©±ã¯ conversation/ ã«å§”è­²
"""
from __future__ import annotations

from pathlib import Path
from typing import Iterable
from transformers import AutoTokenizer
import streamlit as st

# === å±¤ã® import ===
from app.pdf.rag.retrieval_chain_builder import build_retrieval_qa_chain
from ui.sidebar import draw_sidebar, inject_base_style
from ui.chat_window import (
    render_header,
    render_chat_history,
    render_user_input,
    ChatMessage,
    StreamlitTokenSink,
)

from conversation.session_state import ensure_defaults
from conversation.manager import save_conversation, iter_conversations, load_conversation

from model.factory import load_model
from pdf.embedding_manager import load_embeddings

# === å®šæ•° ===
ROOT = Path(__file__).resolve().parent / "../../"
MODELS_DIR = ROOT / "models"
UPLOADED_DOCS_DIR = ROOT / "uploaded_docs"
CONVERSATIONS_DIR = ROOT / "conversations"
VECTOR_CACHE_BASE = ROOT / "faiss_index_cache"
STYLE = ROOT / "ui" / "style.css"
EMBEDDING_MODEL = "intfloat/multilingual-e5-large"  # æ—¢å­˜è¨­è¨ˆã®ç¶™æ‰¿ï¼ˆEmbeddingåï¼‰


def _mkdirs() -> None:
    """å¿…è¦ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ"""
    for d in (MODELS_DIR, UPLOADED_DOCS_DIR, CONVERSATIONS_DIR, VECTOR_CACHE_BASE):
        d.mkdir(parents=True, exist_ok=True)


# ====== Streamlit åˆæœŸåŒ– ======
st.set_page_config(page_title="PocketAI Mentor", page_icon="ğŸ§ ", layout="wide")
inject_base_style(STYLE)
_mkdirs()
ensure_defaults()


# ====== ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒªã‚½ãƒ¼ã‚¹èª­è¾¼ ======
def on_load_resources(model_ref: str, pdf_names: list[str]) -> None:
    """
    ãƒ¢ãƒ‡ãƒ«ã¨PDFã‚’èª­ã¿è¾¼ã¿ã€RAGãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆã—ã¦ session_state ã«ä¿å­˜ã€‚
    - æ—¢å­˜ app.py ã® load_resources() ç›¸å½“ã ãŒã€pathlibåŒ–ï¼†å±¤åˆ†é›¢ç‰ˆã€‚
    """
    # 1) å‚ç…§ã‚’ãƒ­ãƒ¼ã‚«ãƒ«orIDã«æ­£è¦åŒ–
    ref = model_ref.strip()
    cand = (MODELS_DIR / ref)
    is_local = cand.exists() or Path(ref).exists()
    model_path = cand.resolve() if cand.exists() else Path(ref).resolve() if Path(ref).exists() else None
    model_id_or_path = model_path if model_path else ref
    embeddings = load_embeddings(EMBEDDING_MODEL)

    pdf_paths = [UPLOADED_DOCS_DIR / n for n in pdf_names]

    # 2) HF repo_id ã®å ´åˆã¯ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜å…ˆã‚’æ±ºã‚ã‚‹ï¼ˆä¾‹ï¼šmodels/_hub/<owner>__<repo>ï¼‰
    def _sanitize(repo_id: str) -> str:
        return repo_id.replace("/", "__")
    persist_dir = None if is_local else (MODELS_DIR / "_hub" / _sanitize(ref))
    cache_dir = persist_dir  # ãã®ã¾ã¾cache_dirã‚‚åŒã˜å ´æ‰€ã«

    # 3) å‚ç…§ã®å®Ÿä½“ã‹ã‚‰ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’è‡ªå‹•é¸æŠ
    def _looks_like_hf_id(s: str) -> bool:
        return ("/" in s) and ("\\" not in s) and (":" not in s)
    prefer = "ov"
    tok_ref = None
    if isinstance(model_id_or_path, Path):
        if model_id_or_path.is_file() and model_id_or_path.suffix.lower() == ".gguf":
            prefer = "llamacpp"
        elif model_id_or_path.is_dir():
            # openvino_model.xml ã‚’å«ã‚€ãƒ•ã‚©ãƒ«ãƒ€ã‹ï¼Ÿï¼ˆå†å¸°ã§æ¤œæŸ»ï¼‰
            ov_xml = list(model_id_or_path.rglob("openvino_model.xml"))
            if ov_xml:
                model_id_or_path = ov_xml[0].parent  # IR ãƒ•ã‚©ãƒ«ãƒ€ã®ç›´ä¸‹ã‚’æŒ‡ã™
                prefer = "ov"
            else:
                # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã ãŒIRãŒç„¡ã„â†’ å¤‰æ›ã—ã¦ã„ãªã„ LLM ãƒ•ã‚©ãƒ«ãƒ€ç­‰ã€‚ã“ã“ã§ã¯æ‰±ã‚ãªã„ã€‚
                raise ValueError("OpenVINO ã® IR ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆopenvino_model.xml ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼‰ã€‚")
    else:
        # æ–‡å­—åˆ—ï¼šHF ã® OV ãƒ¢ãƒ‡ãƒ«IDãªã‚‰ãã®ã¾ã¾ OVã€‚Windows çµ¶å¯¾ãƒ‘ã‚¹ã¯ \ ã‚„ : ã‚’å«ã‚€ã®ã§å¼¾ã
        if not _looks_like_hf_id(model_id_or_path):
            raise ValueError(f"ãƒ¢ãƒ‡ãƒ«å‚ç…§ãŒä¸æ­£ã§ã™: {model_id_or_path}")

    # 4) ãƒ­ãƒ¼ãƒ‰ï¼ˆprefer ã«å¿œã˜ã¦åˆ‡æ›¿ï¼‰
    if prefer == "llamacpp":
        llm = load_model(
            "llamacpp",
            model_id_or_path,
            n_ctx=2048, n_gpu_layers=0, n_batch=512, f16_kv=True,
        )
        # ãƒˆãƒ¼ã‚¯ãƒ³åˆ†å‰²ç”¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã¯åˆ¥é€”æŒ‡å®šï¼ˆGGUFã¯ç›´æ¥ã¯å–ã‚Œãªã„ãŸã‚ï¼‰
        tok_ref = "gpt2"  # â€»æ—¥æœ¬èªç”¨ã«åˆã‚ã›ãŸã„å ´åˆã¯è©²å½“HF IDã«å·®ã—æ›¿ãˆ
    else:
        llm = load_model(
            "ov",
            model_id_or_path,
            device="GPU",
            export=False,
            max_new_tokens=256,
            persist_dir=persist_dir,
            cache_dir=cache_dir,
        )
        tok_ref = str(model_id_or_path) if isinstance(model_id_or_path, Path) else model_id_or_path


    # 3) RAG ãƒã‚§ãƒ¼ãƒ³
    chain = build_retrieval_qa_chain(
        llm=llm,
        embeddings=embeddings,
        pdf_paths=pdf_paths,
        vector_cache_base=VECTOR_CACHE_BASE,
        chunk_size=700,
        chunk_overlap=80,
        k=3,
        tokenizer_id_or_path=tok_ref,
    )

    # 4) ã‚»ãƒƒã‚·ãƒ§ãƒ³åæ˜ ï¼ˆæ—¢å­˜ã¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸åˆæœŸåŒ–ã—ã¦ã„ãŸãŒã€èª­è¾¼ç³»ã§ã¯ä¿æŒã—ãŸã„å ´åˆã‚‚ã‚ã‚‹ã®ã§ä¸Šæ›¸ãã—ãªã„ï¼‰
    st.session_state["qa_chain"] = chain
    st.session_state["selected_model"] = str(model_id_or_path)
    st.session_state["selected_pdfs"] = pdf_names


# ====== ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ–°è¦ä¼šè©± ======
def on_new_conversation() -> None:
    st.session_state["messages"] = []
    st.session_state["qa_chain"] = None
    st.session_state["selected_model"] = ""
    st.session_state["selected_pdfs"] = []

# ====== ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯: ä¼šè©±ä¿å­˜ ======
def on_save_conversation_cb(save_name: str) -> None:
    save_conversation(CONVERSATIONS_DIR, save_name or None)

# ====== ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯: ä¼šè©±èª­è¾¼ ======
def on_load_conversation_cb(path: Path) -> None:
    """
    1) JSON ã‚’èª­ã¿è¾¼ã¿ session_state ã« model/pdf/messages ã‚’åæ˜ 
    2) RAG ã‚’å†æ§‹ç¯‰ï¼ˆæ—¢å­˜ãƒ•ãƒ­ãƒ¼ã®ã€Œèª­è¾¼â†’load_resourcesâ†’messageså¾©å…ƒã€ã‚’å†ç¾ï¼‰
    """
    load_conversation(CONVERSATIONS_DIR, path)
    model_name = st.session_state.get("selected_model", "")
    pdf_names = list(st.session_state.get("selected_pdfs", []))
    if model_name and pdf_names:
        # å†æ§‹ç¯‰ï¼ˆã“ã“ã§ã¯ messages ã¯ç¶­æŒï¼‰
        on_load_resources(model_name, pdf_names)
        st.rerun()

# ====== ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒãƒ£ãƒƒãƒˆé€ä¿¡ ======
def on_submit(prompt: str, sink: StreamlitTokenSink) -> None:
    # â˜… ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•ã‚’ãƒˆãƒ¼ã‚¯ãƒ³ã§å¼·åˆ¶çŸ­ç¸®ï¼ˆä¾‹ï¼š200 tokensï¼‰
    tok = st.session_state.get("tokenizer", None)
    if tok is not None:
        ids = tok.encode(prompt, add_special_tokens=False)
        if len(ids) > 200:
            head = tok.decode(ids[:140])
            tail = tok.decode(ids[-60:])
            prompt = head + "\nâ€¦ï¼ˆä¸­ç•¥ï¼‰â€¦\n" + tail
    st.session_state["messages"].append({"role": "user", "content": prompt})

    # QA ãƒã‚§ãƒ¼ãƒ³å®Ÿè¡Œï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°APIãŒç„¡ã„æƒ³å®šãªã®ã§ã€ä¸€æ‹¬â†’sink.writeï¼‰
    chain = st.session_state.get("qa_chain")
    if not chain:
        sink.write("ï¼ˆã¾ã PDF/ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ï¼‰")
        return

    # LangChain RetrievalQA.invoke() ã‚’ä½¿ã†ï¼ˆæ—¢å­˜ã®å‘¼ã³å‡ºã—æ–¹ã‚’è¸è¥²ï¼‰
    result = chain.invoke(prompt)
    answer = (result.get("result") or "").strip() or "çµæœã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
    sink.write(answer)

    # å±¥æ­´ã«ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆç™ºè©±ã‚’åæ˜ 
    st.session_state["messages"].append({"role": "assistant", "content": answer})

# ====== UIï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼‰ ======
draw_sidebar(
    models_dir=MODELS_DIR,
    uploaded_docs_dir=UPLOADED_DOCS_DIR,
    conversations_dir=CONVERSATIONS_DIR,
    on_load_resources=on_load_resources,
    on_new_conversation=on_new_conversation,
    on_save_conversation=on_save_conversation_cb,
    on_load_conversation=on_load_conversation_cb,
)

# ====== ãƒ¡ã‚¤ãƒ³ï¼ˆãƒãƒ£ãƒƒãƒˆï¼‰ ======
selected_model = st.session_state.get("selected_model")
selected_pdfs = st.session_state.get("selected_pdfs")

if selected_model and selected_pdfs:
    render_header(selected_model, selected_pdfs)
    render_chat_history(st.session_state.get("messages", []))  # éå»ãƒ­ã‚°
    render_user_input("PDFã«é–¢ã™ã‚‹è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", on_submit)
else:
    st.header("ã‚ˆã†ã“ãï¼ PocketAI Mentorã¸")
    st.markdown(
        "å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ **ãƒ¢ãƒ‡ãƒ«** ã¨ **PDF** ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã—ã€`èª­ã¿è¾¼ã‚€` ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚"
    )
