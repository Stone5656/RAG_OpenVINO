# -*- coding: utf-8 -*-
"""
RAG チェーン組み立てモジュール（予算付き：長文を“外で捌く”構成）

- 役割:
    * Loader → Splitter → Embedding → FAISS → Retriever(MMR/圧縮/再ランク) → Prompt → RetrievalQA(map_reduce)
- 設計:
    * 入力長（コンテキスト長）を増やさずに長文(=総4,000トークン級)を段階的に処理。
    * ベースの Retriever を MMR で多様化し、クロスエンコーダ再ランク＋文脈圧縮で最終投入を絞る。
    * 連結は map_reduce で分割要約→縮約。
- 依存:
    * langchain.chains.RetrievalQA
    * langchain.prompts.PromptTemplate
    * （任意）langchain.retrievers.ContextualCompressionRetriever
    * （任意）langchain.retrievers.document_compressors.CrossEncoderReranker
    * （任意）langchain_community.cross_encoders.HuggingFaceCrossEncoder
"""
from __future__ import annotations

from pathlib import Path
from typing import Iterable, Sequence, Any
# 先頭付近の import に追加
from typing import List, Optional
from langchain_core.documents import Document
from langchain_core.retrievers import BaseRetriever
from langchain_core.callbacks import CallbackManagerForRetrieverRun

# Pydantic v2 / v1 互換の arbitrary_types_allowed 設定
try:
    from pydantic import ConfigDict  # v2
    _MODEL_CONFIG = {"model_config": ConfigDict(arbitrary_types_allowed=True)}
except Exception:  # v1
    _MODEL_CONFIG = {"Config": type("Config", (), {"arbitrary_types_allowed": True})}

class BudgetedRetriever(BaseRetriever):
    # --- Pydantic の「フィールド宣言」(必須) ---
    base: BaseRetriever
    tok: Any  # transformers.PreTrainedTokenizerBase でもOK
    budget_tokens: int = 800

    # v2/v1 互換のために model_config か Config を付与
    locals().update(_MODEL_CONFIG)

    # 同期版
    def _get_relevant_documents(
        self,
        query: str,
        *,
        run_manager: Optional[CallbackManagerForRetrieverRun] = None,
    ) -> List[Document]:
        docs = self.base.get_relevant_documents(
            query, callbacks=run_manager.get_child() if run_manager else None
        )
        return self._trim_to_budget(docs)

    # 非同期版（必要なら）
    async def _aget_relevant_documents(
        self,
        query: str,
        *,
        run_manager: Optional[CallbackManagerForRetrieverRun] = None,
    ) -> List[Document]:
        docs = await self.base.aget_relevant_documents(
            query, callbacks=run_manager.get_child() if run_manager else None
        )
        return self._trim_to_budget(docs)

    # 予算で切り詰める共通ロジック
    def _trim_to_budget(self, docs: List[Document]) -> List[Document]:
        out: List[Document] = []
        used = 0
        for d in docs:
            ids = self.tok.encode(d.page_content, add_special_tokens=False)
            L = len(ids)
            if used + L > self.budget_tokens:
                remain = self.budget_tokens - used
                if remain > 0:
                    trimmed = self.tok.decode(ids[:remain])
                    out.append(Document(page_content=trimmed, metadata=d.metadata))
                break
            out.append(d)
            used += L
        return out

from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_huggingface import HuggingFaceEmbeddings

from .loader import load_pdfs
from transformers import AutoTokenizer
from langchain_text_splitters import RecursiveCharacterTextSplitter
from .vectorstore_manager import cache_dir_for, build_or_load_faiss


# ---- Prompts --------------------------------------------------------------

# 各チャンクに対する“部分回答/要約”用（map フェーズ）
MAP_PROMPT = PromptTemplate(
    template=(
        "### 指示:\n"
        "与えられたコンテキストだけに基づき、ユーザーの質問に答えるための要点を短く日本語で抽出してください。\n"
        "不明な点は「不明」と記してください。推測はしないでください。\n\n"
        "### コンテキスト:\n{context}\n\n"
        "### 質問:\n{question}\n\n"
        "### 要点（箇条書きで可）:\n"
    ),
    input_variables=["context", "question"],
)

# map の結果を統合する“縮約”用（reduce フェーズ）
COMBINE_PROMPT = PromptTemplate(
    template=(
        "### 指示:\n"
        "以下は複数コンテキストから抽出された要点です。重複を除き、矛盾があれば明記し、"
        "ユーザーの質問に対する最終回答を日本語で簡潔にまとめてください。\n"
        "根拠となる情報がなければ「分かりません」と述べてください。\n\n"
        "### 要点一覧:\n{summaries}\n\n"
        "### 質問:\n{question}\n\n"
        "### 最終回答:\n"
    ),
    # ← map_reduce の reduce 側は既定で {summaries} を受け取る
    input_variables=["summaries", "question"],
)

# “stuff” 用のプレーン QA プロンプト（フォールバック）
DEFAULT_QA_PROMPT = PromptTemplate(
    template=(
        "### 指示:\n"
        "以下のコンテキストだけに基づいて日本語で回答してください。"
        "根拠が無い場合は「分かりません」と答えてください。\n\n"
        "### コンテキスト:\n{context}\n\n"
        "### 質問:\n{question}\n\n"
        "### 回答:\n"
    ),
    input_variables=["context", "question"],
)


# ---- Builder --------------------------------------------------------------

def build_retrieval_qa_chain(
    *,
    llm: Any,  # LangChain 互換 LLM（OpenVINO/HF Pipeline/LlamaCpp 等）
    embeddings: HuggingFaceEmbeddings,
    pdf_paths: Iterable[Path],
    vector_cache_base: Path,
    # 分割
    chunk_size: int = 700,
    chunk_overlap: int = 80,
    # 取得/再ランク
    k: int = 3,                    # LLM に最終投入するドキュメント件数
    mmr_fetch_k: int = 40,         # まず広く拾う件数（MMR）
    mmr_lambda: float = 0.3,       # 多様性寄り
    use_cross_encoder: bool = True,
    reranker_model_name: str = "BAAI/bge-reranker-base",
    # 連結
    use_map_reduce: bool = True,   # True: map_reduce / False: stuff
    tokenizer_id_or_path: str | Path | None = None
) -> RetrievalQA:
    """
    取得→圧縮→map_reduce で、入力長を増やさず“長文をはける”構成にする。
    """

    # 1) 読み込み
    docs = load_pdfs(pdf_paths)

    # 2) 分割（小さめチャンクでリコールを確保）
    if tokenizer_id_or_path is not None:
        tok = AutoTokenizer.from_pretrained(str(tokenizer_id_or_path), trust_remote_code=True)
        # ★ モデルと同じトークナイザで“トークン数”として分割（日本語の膨張を考慮）
        splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(
            tok, chunk_size=350, chunk_overlap=40  # ← まずは堅めに
        )
        chunks = splitter.split_documents(docs)
    else:
        # フォールバック（将来削除推奨）
        from .splitter import split_documents
        chunks = split_documents(docs, chunk_size=500, chunk_overlap=60)  # ← 少し厳しめ

    # 3) ベクトルストア（キャッシュ）
    cache_dir = cache_dir_for(pdf_paths, vector_cache_base)
    db = build_or_load_faiss(documents=chunks, embeddings=embeddings, cache_dir=cache_dir)

    # 4) Retriever（MMR で冗長回避 & 多様性確保）
    #    まずは広く fetch し、その後で絞る
    base_retriever = db.as_retriever(
        search_type="mmr",
        search_kwargs={
            "k": max(k * 2, 6),
            "fetch_k": min(mmr_fetch_k, 32),
            "lambda_mult": mmr_lambda,
        },
    )

    retriever = base_retriever

    # ★ 最終安全弁：合計入力を物理的に 800 tokens 以内に制限
    if tokenizer_id_or_path is not None:
        retriever = BudgetedRetriever(base=retriever, tok=tok, budget_tokens=800)

    # 4.5) （任意）クロスエンコーダ再ランク ＋ 文脈圧縮
    #      top_n=k に絞り、LLM への最終投入を制限
    if use_cross_encoder:
        try:
            from langchain.retrievers import ContextualCompressionRetriever
            from langchain.retrievers.document_compressors import CrossEncoderReranker
            from langchain_community.cross_encoders import HuggingFaceCrossEncoder

            rerank_model = HuggingFaceCrossEncoder(model_name=reranker_model_name)
            compressor = CrossEncoderReranker(model=rerank_model, top_n=k)
            # ★ ここで "base_retriever" に包装済み retriever を渡す（上書き回避）
            retriever = ContextualCompressionRetriever(
                base_retriever=retriever,
                base_compressor=compressor,
            )
        except Exception as e:
            # 依存未導入・モデルDL不可などの場合は素の MMR にフォールバック
            # （ログだけ出して継続）
            import warnings
            warnings.warn(f"CrossEncoderReranker を無効化します: {e}")
            retriever = base_retriever

    # 5) RetrievalQA chain（map_reduce で分割要約→縮約）
    if use_map_reduce:
        chain = RetrievalQA.from_chain_type(
            llm=llm,
            retriever=retriever,
            chain_type="map_reduce",
            return_source_documents=False,
            chain_type_kwargs={
                "question_prompt": MAP_PROMPT,   # map 側は既定で {context}
                "combine_prompt": COMBINE_PROMPT, # combine 側は {summaries}
                "token_max": 800,                 # ★ 中間が長いと自動で「折り畳み」てから結合
            },
        )
    else:
        chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True,
            chain_type_kwargs={"prompt": DEFAULT_QA_PROMPT, "document_variable_name": "context"},
    )

    return chain
